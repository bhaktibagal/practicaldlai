import tensorflow as tf
from tensorflow.keras.applications import VGG16
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img,img_to_array
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.callbacks import EarlyStopping




dataset_dir = r"C:\Users\bhakt\OneDrive\Desktop\archive (6)"



dataset_datagen = ImageDataGenerator(rescale=1.0 / 255)


batch_size = 64 # Reduced batch size


dataset_generator = dataset_datagen.flow_from_directory(
dataset_dir,
target_size=(64, 64),
batch_size=batch_size,
class_mode='categorical'
)


x_train, y_train = dataset_generator[0]
x_test, y_test = dataset_generator[1]



base_model = VGG16(weights='imagenet', include_top=False, input_shape=(64, 64,3))


# Step b: Freeze lower layers of the model
for layer in base_model.layers:
    layer.trainable = False



# Step c: Add custom classifier on top
x = Flatten()(base_model.output)
x = Dense(64, activation='relu')(x)
predictions = Dense(len(dataset_generator.class_indices), activation='softmax')(x) # Use t
# Create and compile the model
model = Model(inputs=base_model.input, outputs=predictions)
model.compile(optimizer="adam", loss='categorical_crossentropy', metrics=['accuracy'])
# Step d: Implement early stopping
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)






# Train the model and store history
history = model.fit(
    x_train, y_train, 
    batch_size=batch_size, 
    epochs=5, 
    validation_data=(x_test, y_test)
)

# Optional: you can also add callbacks for better control
# For example, saving the best model or early stopping
# callbacks=[early_stopping, model_checkpoint]

# Access training history data if needed
loss = history.history['loss']
val_loss = history.history['val_loss']
accuracy = history.history.get('accuracy')
val_accuracy = history.history.get('val_accuracy')







# Plot training & validation accuracy
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(loc='upper left')






# Plot training & validation loss
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(loc='upper left')
plt.show()







# Step e: Fine-tune hyperparameters and unfreeze more layers if necessary
for layer in base_model.layers[-4:]:
    layer.trainable = True




# Update the classifier
x = Flatten()(base_model.output)
x = Dense(512, activation='relu')(x)
x = tf.keras.layers.Dropout(0.3)(x)
predictions = Dense(len(dataset_generator.class_indices), activation='softmax')(x)




# Create and compile the fine-tuned model
model = Model(inputs=base_model.input, outputs=predictions)
model.compile(optimizer=Adam(learning_rate=0.001), 
              loss='categorical_crossentropy', 
              metrics=['accuracy'])






# Load and preprocess an external image for prediction
image_path = r'C:\Users\bhakt\OneDrive\Desktop\archive (6)\caltech-101\strawberry\image_0015.jpg'

img = load_img(image_path, target_size=(64, 64)) 








# Train the fine-tuned model with early stopping
history_fine_tune = model.fit(x_train, y_train, batch_size=batch_size, epochs
=5, validation_data=(x_test, y_test), callbacks=[early_stopping])






# Visualize the image and the prediction
plt.imshow(img)
#plt.title(f"Predicted: {predicted_class}")
plt.axis('off')
plt.show()








